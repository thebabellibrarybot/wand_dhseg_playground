{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr_64waaxq8h",
        "outputId": "ab8742a4-2537-444c-a2a6-d76cfb24c12b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement llamba-index (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for llamba-index\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.170)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.6)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA, ConversationalRetrievalChain\n",
        "import os\n"
      ],
      "metadata": {
        "id": "4saNLCcnxxge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# args\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"API_KEY\\n",
        "local_repo_path = '/content/dhSegment'\n",
        "readthedocs_question = ['url array']\n",
        "wand_readthedocs = ['url array']\n"
      ],
      "metadata": {
        "id": "yfb45iHfyVHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dhlab-epfl/dhSegment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTe5kO4tx0yJ",
        "outputId": "d7bf9395-f1bf-45f8-a638-a568869d0d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dhSegment'...\n",
            "remote: Enumerating objects: 2536, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 2536 (delta 0), reused 1 (delta 0), pack-reused 2530\u001b[K\n",
            "Receiving objects: 100% (2536/2536), 6.04 MiB | 24.05 MiB/s, done.\n",
            "Resolving deltas: 100% (1637/1637), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add repo_in_question\n",
        "\n",
        "repo_path = local_repo_path\n",
        "\n",
        "docs = []\n",
        "for dirpath, dirnames, filenames in os.walk(repo_path):\n",
        "    for file in filenames:\n",
        "        if file.endswith('.py') and '/.venv/' not in dirpath:\n",
        "            try: \n",
        "                loader = TextLoader(os.path.join(dirpath, file), encoding='utf-8')\n",
        "                docs.extend(loader.load_and_split())\n",
        "            except Exception as e: \n",
        "                pass\n",
        "print(f'{len(docs)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4jQvqFJx1JX",
        "outputId": "97973025-0790-484b-a122-6e45a2173a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add readthedocs_in_question\n",
        "\n"
      ],
      "metadata": {
        "id": "SftZTK9WodsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add wandb_docs_helper\n"
      ],
      "metadata": {
        "id": "MAjLOa0UoloQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine repo_in_question, readthedocs_in_question, wandb_docs_helper\n"
      ],
      "metadata": {
        "id": "GgnkjyJ0pcxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbO6u9jxx2tW",
        "outputId": "336ddfbb-c114-456e-bbfd-ac92d0103d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1200, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1224, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1015, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1400, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2030, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1489, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 2950, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1105, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1167, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1264, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1068, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1367, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1152, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(docs, embeddings)\n"
      ],
      "metadata": {
        "id": "TUYnqtZMx9mF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = db.as_retriever()\n",
        "retriever.search_kwargs['distance_metric'] = 'cos'\n",
        "retriever.search_kwargs['fetch_k'] = 20\n",
        "retriever.search_kwargs['maximal_marginal_relevance'] = True\n",
        "retriever.search_kwargs['k'] = 20"
      ],
      "metadata": {
        "id": "rSPXhPPeiLpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatOpenAI(model_name='gpt-3.5-turbo')\n",
        "qa = ConversationalRetrievalChain.from_llm(model,retriever=retriever)"
      ],
      "metadata": {
        "id": "L6m4RGtPyG7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"how do i extract text lines with dhsegment\",\n",
        "    \"give me an example of how to do this\"\n",
        "] \n",
        "chat_history = []\n",
        "\n",
        "for question in questions:  \n",
        "    result = qa({\"question\": question, \"chat_history\": chat_history})\n",
        "    chat_history.append((question, result['answer']))\n",
        "    print(f\"-> **Question**: {question} \\n\")\n",
        "    print(f\"**Answer**: {result['answer']} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qi1rwVzCykH_",
        "outputId": "a2f03dff-6f7c-4555-a56a-2a76618775c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-> **Question**: how do i extract text lines with dhsegment \n",
            "\n",
            "**Answer**: The dhsegment package provides a framework for historical document processing. To extract text lines with dhsegment, you can use the `page_extraction` function provided in the code you have. This function takes an input image and outputs the text lines found in the image in a PAGEXML file format.\n",
            "\n",
            "Here are the steps you can follow to use `page_extraction`:\n",
            "\n",
            "1. Install dhsegment by running `pip install dhsegment` in your terminal.\n",
            "2. Import the `page_extraction` function by adding `from dh_segment.post_processing import page_extraction` at the top of your Python file.\n",
            "3. Call `page_extraction` with the following arguments:\n",
            "   - `model_dir`: The directory of the model to use for text line extraction.\n",
            "   - `filenames_to_process`: A list of image file paths to extract text lines from.\n",
            "   - `output_dir`: The directory to output the PAGEXML files.\n",
            "   - `draw_extractions`: If true, the extracted lines will be drawn and exported to the output_dir.\n",
            "\n",
            "Here's an example code snippet:\n",
            "\n",
            "```python\n",
            "from dh_segment.post_processing import page_extraction\n",
            "\n",
            "model_dir = 'path/to/model'\n",
            "filenames_to_process = ['path/to/image1.png', 'path/to/image2.png']\n",
            "output_dir = 'path/to/output'\n",
            "draw_extractions = True\n",
            "\n",
            "page_extraction(model_dir, filenames_to_process, output_dir, draw_extractions)\n",
            "```\n",
            "\n",
            "This will extract text lines from the images specified in `filenames_to_process`, using the model located in `model_dir`, and output the results in the `output_dir` directory. If `draw_extractions` is set to `True`, the extracted lines will also be drawn and exported as images. \n",
            "\n",
            "-> **Question**: give me an example of how to do this \n",
            "\n",
            "**Answer**: The given code shows the implementation of dhsegment model. It does not show the steps to extract text lines with dhsegment. However, I can tell you that dhSegment is a python package developed for historical document segmentation and processing. To extract text lines with dhsegment, you'll need to use a pre-trained model that can be downloaded from the dhlab-epfl/dhSegment GitHub repository. Once you have the model, you can use it to extract text lines from an image by providing the image to the model. You can do so by calling the predict() method on the LoadedModel object and passing the filename of the image to process. The output of the model will be a prediction object containing the probabilities of each pixel being part of a text line. You can then apply post-processing techniques to these predictions to extract the text lines. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5pFqvVVNiSiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zYvuQHTY6nH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "bib:\n",
        "\n",
        "langchain_docs:\n",
        "  git_loader: https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/git.html\n",
        "  readthedocs_loader: https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/readthedocs_documentation.html\n",
        "\n",
        "  \n",
        "\"\"\""
      ],
      "metadata": {
        "id": "stkVdiUU6niR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
